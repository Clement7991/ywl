{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "# imports and setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import os \n",
    "\n",
    "# import and instanciate config file to get access to paths\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "from config.config import Config\n",
    "config = Config()\n",
    "\n",
    "# import utils functions\n",
    "from src.utils.process_books import process_books\n",
    "from src.pipelines.preprocessor import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twain Mark</td>\n",
       "      <td>﻿The Project Gutenberg EBook of Chapters from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Plato</td>\n",
       "      <td>﻿The Project Gutenberg EBook of Sophist, by Pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twain Mark</td>\n",
       "      <td>﻿The Project Gutenberg EBook of On the Decay o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dante Alighieri</td>\n",
       "      <td>﻿The Project Gutenberg eBook of Pericles, by W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shelley Mary Wollstonecraft</td>\n",
       "      <td>﻿The Project Gutenberg eBook of As You Like It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>Various</td>\n",
       "      <td>﻿The Project Gutenberg EBook of Life On The Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>Twain Mark</td>\n",
       "      <td>﻿The Project Gutenberg EBook of A Tramp Abroad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>Austen Jane</td>\n",
       "      <td>﻿The Project Gutenberg eBook of Sense and Sens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>Luther Martin</td>\n",
       "      <td>﻿\\nProject Gutenberg Etext of The Rape of Lucr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>Burnett Frances Hodgson</td>\n",
       "      <td>﻿The Project Gutenberg EBook of Statesman, by ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>436 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Author  \\\n",
       "0                     Twain Mark   \n",
       "1                          Plato   \n",
       "2                     Twain Mark   \n",
       "3                Dante Alighieri   \n",
       "4    Shelley Mary Wollstonecraft   \n",
       "..                           ...   \n",
       "431                      Various   \n",
       "432                   Twain Mark   \n",
       "433                  Austen Jane   \n",
       "434                Luther Martin   \n",
       "435      Burnett Frances Hodgson   \n",
       "\n",
       "                                                  Book  \n",
       "0    ﻿The Project Gutenberg EBook of Chapters from ...  \n",
       "1    ﻿The Project Gutenberg EBook of Sophist, by Pl...  \n",
       "2    ﻿The Project Gutenberg EBook of On the Decay o...  \n",
       "3    ﻿The Project Gutenberg eBook of Pericles, by W...  \n",
       "4    ﻿The Project Gutenberg eBook of As You Like It...  \n",
       "..                                                 ...  \n",
       "431  ﻿The Project Gutenberg EBook of Life On The Mi...  \n",
       "432  ﻿The Project Gutenberg EBook of A Tramp Abroad...  \n",
       "433  ﻿The Project Gutenberg eBook of Sense and Sens...  \n",
       "434  ﻿\\nProject Gutenberg Etext of The Rape of Lucr...  \n",
       "435  ﻿The Project Gutenberg EBook of Statesman, by ...  \n",
       "\n",
       "[436 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "\n",
    "df_init = read_csv(config.DATA_PROCESSED_PATH + 'books_processed.csv')\n",
    "df_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipelines.preprocessor import Preprocessor\n",
    "\n",
    "preprocessor = Preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twain Mark</td>\n",
       "      <td>Produced by Betsie Bush, Chuck Greif, Martin P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Plato</td>\n",
       "      <td>Produced by Sue Asscher\\n\\n\\n\\n\\n\\nSOPHIST\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twain Mark</td>\n",
       "      <td>ON THE DECAY OF THE ART OF LYING\\n\\nby Mark Tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dante Alighieri</td>\n",
       "      <td>cover \\n\\n\\n\\nPERICLES, PRINCE OF TYRE\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shelley Mary Wollstonecraft</td>\n",
       "      <td>cover \\n\\n\\n\\n\\nAS YOU LIKE IT\\n\\nby William S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>Various</td>\n",
       "      <td>Produced by David Widger\\n\\n\\n\\n\\n\\n          ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>Twain Mark</td>\n",
       "      <td>Produced by Anonymous Volunteers, John Greenma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>Austen Jane</td>\n",
       "      <td>Transcriber's Note:\\n\\nThe Table of Contents i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>Luther Martin</td>\n",
       "      <td>﻿\\nProject Gutenberg Etext of The Rape of Lucr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>Burnett Frances Hodgson</td>\n",
       "      <td>Produced by Sue Asscher\\n\\n\\n\\n\\n\\nSTATESMAN\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>436 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Author  \\\n",
       "0                     Twain Mark   \n",
       "1                          Plato   \n",
       "2                     Twain Mark   \n",
       "3                Dante Alighieri   \n",
       "4    Shelley Mary Wollstonecraft   \n",
       "..                           ...   \n",
       "431                      Various   \n",
       "432                   Twain Mark   \n",
       "433                  Austen Jane   \n",
       "434                Luther Martin   \n",
       "435      Burnett Frances Hodgson   \n",
       "\n",
       "                                                  Book  \n",
       "0    Produced by Betsie Bush, Chuck Greif, Martin P...  \n",
       "1    Produced by Sue Asscher\\n\\n\\n\\n\\n\\nSOPHIST\\n\\n...  \n",
       "2    ON THE DECAY OF THE ART OF LYING\\n\\nby Mark Tw...  \n",
       "3    cover \\n\\n\\n\\nPERICLES, PRINCE OF TYRE\\n\\n\\n\\n...  \n",
       "4    cover \\n\\n\\n\\n\\nAS YOU LIKE IT\\n\\nby William S...  \n",
       "..                                                 ...  \n",
       "431  Produced by David Widger\\n\\n\\n\\n\\n\\n          ...  \n",
       "432  Produced by Anonymous Volunteers, John Greenma...  \n",
       "433  Transcriber's Note:\\n\\nThe Table of Contents i...  \n",
       "434  ﻿\\nProject Gutenberg Etext of The Rape of Lucr...  \n",
       "435  Produced by Sue Asscher\\n\\n\\n\\n\\n\\nSTATESMAN\\n...  \n",
       "\n",
       "[436 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocessor.body_preprocessor(df_init)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twain Mark</td>\n",
       "      <td>produced by betsie bush, chuck greif, martin p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Plato</td>\n",
       "      <td>produced by sue asscher\\n\\n\\n\\n\\n\\nsophist\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twain Mark</td>\n",
       "      <td>on the decay of the art of lying\\n\\nby mark tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dante Alighieri</td>\n",
       "      <td>cover \\n\\n\\n\\npericles, prince of tyre\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shelley Mary Wollstonecraft</td>\n",
       "      <td>cover \\n\\n\\n\\n\\nas you like it\\n\\nby william s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>Various</td>\n",
       "      <td>produced by david widger\\n\\n\\n\\n\\n\\n          ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>Twain Mark</td>\n",
       "      <td>produced by anonymous volunteers, john greenma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>Austen Jane</td>\n",
       "      <td>transcriber's note:\\n\\nthe table of contents i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>Luther Martin</td>\n",
       "      <td>﻿\\nproject gutenberg etext of the rape of lucr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>Burnett Frances Hodgson</td>\n",
       "      <td>produced by sue asscher\\n\\n\\n\\n\\n\\nstatesman\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>436 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Author  \\\n",
       "0                     Twain Mark   \n",
       "1                          Plato   \n",
       "2                     Twain Mark   \n",
       "3                Dante Alighieri   \n",
       "4    Shelley Mary Wollstonecraft   \n",
       "..                           ...   \n",
       "431                      Various   \n",
       "432                   Twain Mark   \n",
       "433                  Austen Jane   \n",
       "434                Luther Martin   \n",
       "435      Burnett Frances Hodgson   \n",
       "\n",
       "                                                  Book  \n",
       "0    produced by betsie bush, chuck greif, martin p...  \n",
       "1    produced by sue asscher\\n\\n\\n\\n\\n\\nsophist\\n\\n...  \n",
       "2    on the decay of the art of lying\\n\\nby mark tw...  \n",
       "3    cover \\n\\n\\n\\npericles, prince of tyre\\n\\n\\n\\n...  \n",
       "4    cover \\n\\n\\n\\n\\nas you like it\\n\\nby william s...  \n",
       "..                                                 ...  \n",
       "431  produced by david widger\\n\\n\\n\\n\\n\\n          ...  \n",
       "432  produced by anonymous volunteers, john greenma...  \n",
       "433  transcriber's note:\\n\\nthe table of contents i...  \n",
       "434  ﻿\\nproject gutenberg etext of the rape of lucr...  \n",
       "435  produced by sue asscher\\n\\n\\n\\n\\n\\nstatesman\\n...  \n",
       "\n",
       "[436 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocessor.clean_body_preprocessor(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "427 chunks created\n",
      "176 chunks created\n",
      "8 chunks created\n",
      "77 chunks created\n",
      "90 chunks created\n",
      "80 chunks created\n",
      "88 chunks created\n",
      "55 chunks created\n",
      "636 chunks created\n",
      "3757 chunks created\n",
      "285 chunks created\n",
      "12 chunks created\n",
      "116 chunks created\n",
      "119 chunks created\n",
      "535 chunks created\n",
      "39 chunks created\n",
      "84 chunks created\n",
      "96 chunks created\n",
      "275 chunks created\n",
      "364 chunks created\n",
      "46 chunks created\n",
      "91 chunks created\n",
      "82 chunks created\n",
      "84 chunks created\n",
      "57 chunks created\n",
      "95 chunks created\n",
      "104 chunks created\n",
      "87 chunks created\n",
      "92 chunks created\n",
      "431 chunks created\n",
      "844 chunks created\n",
      "110 chunks created\n",
      "36 chunks created\n",
      "115 chunks created\n",
      "540 chunks created\n",
      "89 chunks created\n",
      "32 chunks created\n",
      "540 chunks created\n",
      "115 chunks created\n",
      "86 chunks created\n",
      "103 chunks created\n",
      "121 chunks created\n",
      "32 chunks created\n",
      "111 chunks created\n",
      "207 chunks created\n",
      "617 chunks created\n",
      "52 chunks created\n",
      "26 chunks created\n",
      "75 chunks created\n",
      "79 chunks created\n",
      "115 chunks created\n",
      "107 chunks created\n",
      "258 chunks created\n",
      "68 chunks created\n",
      "663 chunks created\n",
      "38 chunks created\n",
      "505 chunks created\n",
      "69 chunks created\n",
      "88 chunks created\n",
      "23 chunks created\n",
      "109 chunks created\n",
      "312 chunks created\n",
      "33 chunks created\n",
      "843 chunks created\n",
      "4 chunks created\n",
      "31 chunks created\n",
      "94 chunks created\n",
      "248 chunks created\n",
      "56 chunks created\n",
      "96 chunks created\n",
      "75 chunks created\n",
      "91 chunks created\n",
      "432 chunks created\n",
      "27 chunks created\n",
      "89 chunks created\n",
      "108 chunks created\n",
      "78 chunks created\n",
      "59 chunks created\n",
      "15 chunks created\n",
      "70 chunks created\n",
      "41 chunks created\n",
      "1 chunks created\n",
      "41 chunks created\n",
      "96 chunks created\n",
      "24 chunks created\n",
      "124 chunks created\n",
      "10 chunks created\n",
      "108 chunks created\n",
      "440 chunks created\n",
      "256 chunks created\n",
      "650 chunks created\n",
      "70 chunks created\n",
      "552 chunks created\n",
      "108 chunks created\n",
      "83 chunks created\n",
      "35 chunks created\n",
      "60 chunks created\n",
      "41 chunks created\n",
      "51 chunks created\n",
      "120 chunks created\n",
      "325 chunks created\n",
      "133 chunks created\n",
      "139 chunks created\n",
      "108 chunks created\n",
      "140 chunks created\n",
      "15 chunks created\n",
      "90 chunks created\n",
      "158 chunks created\n",
      "317 chunks created\n",
      "596 chunks created\n",
      "145 chunks created\n",
      "44 chunks created\n",
      "107 chunks created\n",
      "2199 chunks created\n",
      "250 chunks created\n",
      "35 chunks created\n",
      "50 chunks created\n",
      "76 chunks created\n",
      "24 chunks created\n",
      "90 chunks created\n",
      "98 chunks created\n",
      "51 chunks created\n",
      "90 chunks created\n",
      "78 chunks created\n",
      "99 chunks created\n",
      "68 chunks created\n",
      "246 chunks created\n",
      "433 chunks created\n",
      "120 chunks created\n",
      "56 chunks created\n",
      "149 chunks created\n",
      "50 chunks created\n",
      "488 chunks created\n",
      "55 chunks created\n",
      "140 chunks created\n",
      "667 chunks created\n",
      "59 chunks created\n",
      "101 chunks created\n",
      "137 chunks created\n",
      "108 chunks created\n",
      "114 chunks created\n",
      "84 chunks created\n",
      "110 chunks created\n",
      "322 chunks created\n",
      "70 chunks created\n",
      "71 chunks created\n",
      "92 chunks created\n",
      "251 chunks created\n",
      "115 chunks created\n",
      "374 chunks created\n",
      "110 chunks created\n",
      "48 chunks created\n",
      "84 chunks created\n",
      "1386 chunks created\n",
      "186 chunks created\n",
      "604 chunks created\n",
      "1386 chunks created\n",
      "177 chunks created\n",
      "50 chunks created\n",
      "116 chunks created\n",
      "308 chunks created\n",
      "25 chunks created\n",
      "45 chunks created\n",
      "43 chunks created\n",
      "75 chunks created\n",
      "416 chunks created\n",
      "160 chunks created\n",
      "575 chunks created\n",
      "31 chunks created\n",
      "121 chunks created\n",
      "95 chunks created\n",
      "108 chunks created\n",
      "128 chunks created\n",
      "93 chunks created\n",
      "103 chunks created\n",
      "74 chunks created\n",
      "125 chunks created\n",
      "73 chunks created\n",
      "177 chunks created\n",
      "121 chunks created\n",
      "931 chunks created\n",
      "133 chunks created\n",
      "116 chunks created\n",
      "12 chunks created\n",
      "181 chunks created\n",
      "98 chunks created\n",
      "109 chunks created\n",
      "81 chunks created\n",
      "368 chunks created\n",
      "16 chunks created\n",
      "126 chunks created\n",
      "403 chunks created\n",
      "1 chunks created\n",
      "108 chunks created\n",
      "243 chunks created\n",
      "290 chunks created\n",
      "188 chunks created\n",
      "615 chunks created\n",
      "47 chunks created\n",
      "84 chunks created\n",
      "287 chunks created\n",
      "96 chunks created\n",
      "96 chunks created\n",
      "109 chunks created\n",
      "1392 chunks created\n",
      "79 chunks created\n",
      "93 chunks created\n",
      "3192 chunks created\n",
      "106 chunks created\n",
      "114 chunks created\n",
      "115 chunks created\n",
      "115 chunks created\n",
      "77 chunks created\n",
      "48 chunks created\n",
      "354 chunks created\n",
      "87 chunks created\n",
      "28 chunks created\n",
      "64 chunks created\n",
      "51 chunks created\n",
      "108 chunks created\n",
      "75 chunks created\n",
      "44 chunks created\n",
      "76 chunks created\n",
      "496 chunks created\n",
      "712 chunks created\n",
      "342 chunks created\n",
      "12 chunks created\n",
      "90 chunks created\n",
      "89 chunks created\n",
      "103 chunks created\n",
      "484 chunks created\n",
      "250 chunks created\n",
      "117 chunks created\n",
      "108 chunks created\n",
      "76 chunks created\n",
      "108 chunks created\n",
      "80 chunks created\n",
      "50 chunks created\n",
      "1317 chunks created\n",
      "85 chunks created\n",
      "58 chunks created\n",
      "71 chunks created\n",
      "47 chunks created\n",
      "653 chunks created\n",
      "125 chunks created\n",
      "206 chunks created\n",
      "17 chunks created\n",
      "1365 chunks created\n",
      "33 chunks created\n",
      "86 chunks created\n",
      "85 chunks created\n",
      "130 chunks created\n",
      "54 chunks created\n",
      "127 chunks created\n",
      "96 chunks created\n",
      "98 chunks created\n",
      "120 chunks created\n",
      "655 chunks created\n",
      "96 chunks created\n",
      "112 chunks created\n",
      "97 chunks created\n",
      "70 chunks created\n",
      "236 chunks created\n",
      "66 chunks created\n",
      "67 chunks created\n",
      "623 chunks created\n",
      "41 chunks created\n",
      "127 chunks created\n",
      "117 chunks created\n",
      "56 chunks created\n",
      "100 chunks created\n",
      "68 chunks created\n",
      "10884 chunks created\n",
      "68 chunks created\n",
      "420 chunks created\n",
      "52 chunks created\n",
      "309 chunks created\n",
      "96 chunks created\n",
      "96 chunks created\n",
      "92 chunks created\n",
      "754 chunks created\n",
      "110 chunks created\n",
      "97 chunks created\n",
      "98 chunks created\n",
      "96 chunks created\n",
      "92 chunks created\n",
      "106 chunks created\n",
      "45 chunks created\n",
      "81 chunks created\n",
      "400 chunks created\n",
      "299 chunks created\n",
      "114 chunks created\n",
      "49 chunks created\n",
      "59 chunks created\n",
      "459 chunks created\n",
      "22 chunks created\n",
      "486 chunks created\n",
      "51 chunks created\n",
      "74 chunks created\n",
      "43 chunks created\n",
      "65 chunks created\n",
      "114 chunks created\n",
      "90 chunks created\n",
      "87 chunks created\n",
      "71 chunks created\n",
      "306 chunks created\n",
      "95 chunks created\n",
      "108 chunks created\n",
      "91 chunks created\n",
      "105 chunks created\n",
      "94 chunks created\n",
      "119 chunks created\n",
      "30 chunks created\n",
      "51 chunks created\n",
      "76 chunks created\n",
      "111 chunks created\n",
      "132 chunks created\n",
      "109 chunks created\n",
      "118 chunks created\n",
      "90 chunks created\n",
      "38 chunks created\n",
      "99 chunks created\n",
      "3 chunks created\n",
      "355 chunks created\n",
      "83 chunks created\n",
      "191 chunks created\n",
      "33 chunks created\n",
      "38 chunks created\n",
      "12 chunks created\n",
      "13 chunks created\n",
      "84 chunks created\n",
      "79 chunks created\n",
      "96 chunks created\n",
      "103 chunks created\n",
      "555 chunks created\n",
      "77 chunks created\n",
      "225 chunks created\n",
      "100 chunks created\n",
      "8 chunks created\n",
      "101 chunks created\n",
      "31 chunks created\n",
      "138 chunks created\n",
      "301 chunks created\n",
      "82 chunks created\n",
      "54 chunks created\n",
      "92 chunks created\n",
      "101 chunks created\n",
      "330 chunks created\n",
      "135 chunks created\n",
      "74 chunks created\n",
      "94 chunks created\n",
      "74 chunks created\n",
      "88 chunks created\n",
      "37 chunks created\n",
      "0 chunks created\n",
      "7 chunks created\n",
      "122 chunks created\n",
      "91 chunks created\n",
      "80 chunks created\n",
      "27 chunks created\n",
      "105 chunks created\n",
      "87 chunks created\n",
      "96 chunks created\n",
      "31 chunks created\n",
      "617 chunks created\n",
      "209 chunks created\n",
      "104 chunks created\n",
      "81 chunks created\n",
      "98 chunks created\n",
      "74 chunks created\n",
      "739 chunks created\n",
      "338 chunks created\n",
      "93 chunks created\n",
      "17 chunks created\n",
      "71 chunks created\n",
      "92 chunks created\n",
      "10 chunks created\n",
      "91 chunks created\n",
      "85 chunks created\n",
      "31 chunks created\n",
      "63 chunks created\n",
      "33 chunks created\n",
      "107 chunks created\n",
      "89 chunks created\n",
      "97 chunks created\n",
      "23 chunks created\n",
      "40 chunks created\n",
      "113 chunks created\n",
      "362 chunks created\n",
      "277 chunks created\n",
      "63 chunks created\n",
      "96 chunks created\n",
      "32 chunks created\n",
      "90 chunks created\n",
      "573 chunks created\n",
      "115 chunks created\n",
      "231 chunks created\n",
      "93 chunks created\n",
      "96 chunks created\n",
      "213 chunks created\n",
      "112 chunks created\n",
      "108 chunks created\n",
      "78 chunks created\n",
      "92 chunks created\n",
      "91 chunks created\n",
      "88 chunks created\n",
      "61 chunks created\n",
      "184 chunks created\n",
      "154 chunks created\n",
      "30 chunks created\n",
      "85 chunks created\n",
      "58 chunks created\n",
      "102 chunks created\n",
      "44 chunks created\n",
      "977 chunks created\n",
      "38 chunks created\n",
      "84 chunks created\n",
      "22 chunks created\n",
      "1402 chunks created\n",
      "203 chunks created\n",
      "92 chunks created\n",
      "74 chunks created\n",
      "438 chunks created\n",
      "30 chunks created\n",
      "124 chunks created\n",
      "115 chunks created\n",
      "27 chunks created\n",
      "20 chunks created\n",
      "99 chunks created\n",
      "108 chunks created\n",
      "167 chunks created\n",
      "51 chunks created\n",
      "611 chunks created\n",
      "476 chunks created\n",
      "66 chunks created\n",
      "156 chunks created\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twain Mark</td>\n",
       "      <td>produced by betsie bush, chuck greif, martin p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twain Mark</td>\n",
       "      <td>which he sees his own life reflected and set d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twain Mark</td>\n",
       "      <td>believed it would, and asked me if i meant to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Twain Mark</td>\n",
       "      <td>tradition, this one or another--geoffrey cleme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twain Mark</td>\n",
       "      <td>of clemenses who said they had examined the re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92892</th>\n",
       "      <td>Burnett Frances Hodgson</td>\n",
       "      <td>one another, and give rise to a similar opposi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92893</th>\n",
       "      <td>Burnett Frances Hodgson</td>\n",
       "      <td>she desires to create, but only in what will p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92894</th>\n",
       "      <td>Burnett Frances Hodgson</td>\n",
       "      <td>good and their opposites, which is true and co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92895</th>\n",
       "      <td>Burnett Frances Hodgson</td>\n",
       "      <td>medicine which art prescribes for them, and of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92896</th>\n",
       "      <td>Burnett Frances Hodgson</td>\n",
       "      <td>whereas they should both do precisely the oppo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92897 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Author  \\\n",
       "0                   Twain Mark   \n",
       "1                   Twain Mark   \n",
       "2                   Twain Mark   \n",
       "3                   Twain Mark   \n",
       "4                   Twain Mark   \n",
       "...                        ...   \n",
       "92892  Burnett Frances Hodgson   \n",
       "92893  Burnett Frances Hodgson   \n",
       "92894  Burnett Frances Hodgson   \n",
       "92895  Burnett Frances Hodgson   \n",
       "92896  Burnett Frances Hodgson   \n",
       "\n",
       "                                                    Book  \n",
       "0      produced by betsie bush, chuck greif, martin p...  \n",
       "1      which he sees his own life reflected and set d...  \n",
       "2      believed it would, and asked me if i meant to ...  \n",
       "3      tradition, this one or another--geoffrey cleme...  \n",
       "4      of clemenses who said they had examined the re...  \n",
       "...                                                  ...  \n",
       "92892  one another, and give rise to a similar opposi...  \n",
       "92893  she desires to create, but only in what will p...  \n",
       "92894  good and their opposites, which is true and co...  \n",
       "92895  medicine which art prescribes for them, and of...  \n",
       "92896  whereas they should both do precisely the oppo...  \n",
       "\n",
       "[92897 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocessor.chunk_text_preprocessor(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>207</td>\n",
       "      <td>produced by betsie bush, chuck greif, martin p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>207</td>\n",
       "      <td>which he sees his own life reflected and set d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>207</td>\n",
       "      <td>believed it would, and asked me if i meant to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>207</td>\n",
       "      <td>tradition, this one or another--geoffrey cleme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>207</td>\n",
       "      <td>of clemenses who said they had examined the re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92892</th>\n",
       "      <td>33</td>\n",
       "      <td>one another, and give rise to a similar opposi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92893</th>\n",
       "      <td>33</td>\n",
       "      <td>she desires to create, but only in what will p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92894</th>\n",
       "      <td>33</td>\n",
       "      <td>good and their opposites, which is true and co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92895</th>\n",
       "      <td>33</td>\n",
       "      <td>medicine which art prescribes for them, and of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92896</th>\n",
       "      <td>33</td>\n",
       "      <td>whereas they should both do precisely the oppo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92897 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Author                                               Book\n",
       "0         207  produced by betsie bush, chuck greif, martin p...\n",
       "1         207  which he sees his own life reflected and set d...\n",
       "2         207  believed it would, and asked me if i meant to ...\n",
       "3         207  tradition, this one or another--geoffrey cleme...\n",
       "4         207  of clemenses who said they had examined the re...\n",
       "...       ...                                                ...\n",
       "92892      33  one another, and give rise to a similar opposi...\n",
       "92893      33  she desires to create, but only in what will p...\n",
       "92894      33  good and their opposites, which is true and co...\n",
       "92895      33  medicine which art prescribes for them, and of...\n",
       "92896      33  whereas they should both do precisely the oppo...\n",
       "\n",
       "[92897 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocessor.encode_label_preprocessor(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipelines.preproc.preprocv1 import build_preprocessor_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[ColumnTransformer(transformers=[(&#x27;text_pipeline&#x27;,\n",
       "                                                 Pipeline(steps=[(&#x27;body&#x27;,\n",
       "                                                                  FunctionTransformer(func=&lt;bound method Preprocessor.body_preprocessor of &lt;src.pipelines.preprocessor.Preprocessor object at 0x2a6417910&gt;&gt;)),\n",
       "                                                                 (&#x27;clean_body&#x27;,\n",
       "                                                                  FunctionTransformer(func=&lt;bound method Preprocessor.clean_body_preprocessor of &lt;src.pipelines.preprocessor.Preprocessor object at 0x2a6417910&gt;&gt;))])),\n",
       "                                                (&#x27;author_pipeline&#x27;,\n",
       "                                                 Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                                  FunctionTransformer(func=&lt;bound method Preprocessor.encode_label_preprocessor of &lt;src.pipelines.preprocessor.Preprocessor object at 0x2a6417910&gt;&gt;))]))])])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[ColumnTransformer(transformers=[(&#x27;text_pipeline&#x27;,\n",
       "                                                 Pipeline(steps=[(&#x27;body&#x27;,\n",
       "                                                                  FunctionTransformer(func=&lt;bound method Preprocessor.body_preprocessor of &lt;src.pipelines.preprocessor.Preprocessor object at 0x2a6417910&gt;&gt;)),\n",
       "                                                                 (&#x27;clean_body&#x27;,\n",
       "                                                                  FunctionTransformer(func=&lt;bound method Preprocessor.clean_body_preprocessor of &lt;src.pipelines.preprocessor.Preprocessor object at 0x2a6417910&gt;&gt;))])),\n",
       "                                                (&#x27;author_pipeline&#x27;,\n",
       "                                                 Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                                  FunctionTransformer(func=&lt;bound method Preprocessor.encode_label_preprocessor of &lt;src.pipelines.preprocessor.Preprocessor object at 0x2a6417910&gt;&gt;))]))])])</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[ColumnTransformer(transformers=[('text_pipeline',\n",
       "                                                 Pipeline(steps=[('body',\n",
       "                                                                  FunctionTransformer(func=<bound method Preprocessor.body_preprocessor of <src.pipelines.preprocessor.Preprocessor object at 0x2a6417910>>)),\n",
       "                                                                 ('clean_body',\n",
       "                                                                  FunctionTransformer(func=<bound method Preprocessor.clean_body_preprocessor of <src.pipelines.preprocessor.Preprocessor object at 0x2a6417910>>))])),\n",
       "                                                ('author_pipeline',\n",
       "                                                 Pipeline(steps=[('encoder',\n",
       "                                                                  FunctionTransformer(func=<bound method Preprocessor.encode_label_preprocessor of <src.pipelines.preprocessor.Preprocessor object at 0x2a6417910>>))]))])])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import set_config; set_config(display='diagram')\n",
    "pipe = build_preprocessor_pipeline()\n",
    "pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable ColumnTransformer object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipe\u001b[39m.\u001b[39;49mfit_transform(df_init)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ywl/lib/python3.10/site-packages/sklearn/pipeline.py:436\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params):\n\u001b[1;32m    410\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit the model and transform with the final estimator.\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \n\u001b[1;32m    412\u001b[0m \u001b[39m    Fits all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[39m        Transformed samples.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 436\u001b[0m     fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_fit_params(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    437\u001b[0m     Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps)\n\u001b[1;32m    439\u001b[0m     last_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ywl/lib/python3.10/site-packages/sklearn/pipeline.py:320\u001b[0m, in \u001b[0;36mPipeline._check_fit_params\u001b[0;34m(self, **fit_params)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_fit_params\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params):\n\u001b[0;32m--> 320\u001b[0m     fit_params_steps \u001b[39m=\u001b[39m {name: {} \u001b[39mfor\u001b[39;00m name, step \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps \u001b[39mif\u001b[39;00m step \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m}\n\u001b[1;32m    321\u001b[0m     \u001b[39mfor\u001b[39;00m pname, pval \u001b[39min\u001b[39;00m fit_params\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    322\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m__\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pname:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ywl/lib/python3.10/site-packages/sklearn/pipeline.py:320\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_fit_params\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params):\n\u001b[0;32m--> 320\u001b[0m     fit_params_steps \u001b[39m=\u001b[39m {name: {} \u001b[39mfor\u001b[39;00m name, step \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps \u001b[39mif\u001b[39;00m step \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m}\n\u001b[1;32m    321\u001b[0m     \u001b[39mfor\u001b[39;00m pname, pval \u001b[39min\u001b[39;00m fit_params\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    322\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m__\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pname:\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable ColumnTransformer object"
     ]
    }
   ],
   "source": [
    "pipe.fit_transform(df_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>207</td>\n",
       "      <td>produced by betsie bush, chuck greif, martin p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>163</td>\n",
       "      <td>produced by sue asscher\\n\\n\\n\\n\\n\\nsophist\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>207</td>\n",
       "      <td>on the decay of the art of lying\\n\\nby mark tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>cover \\n\\n\\n\\npericles, prince of tyre\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>183</td>\n",
       "      <td>cover \\n\\n\\n\\n\\nas you like it\\n\\nby william s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>210</td>\n",
       "      <td>produced by david widger\\n\\n\\n\\n\\n\\n          ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>207</td>\n",
       "      <td>produced by anonymous volunteers, john greenma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>10</td>\n",
       "      <td>transcriber's note:\\n\\nthe table of contents i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>126</td>\n",
       "      <td>﻿\\nproject gutenberg etext of the rape of lucr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>33</td>\n",
       "      <td>produced by sue asscher\\n\\n\\n\\n\\n\\nstatesman\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>436 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Author                                               Book\n",
       "0       207  produced by betsie bush, chuck greif, martin p...\n",
       "1       163  produced by sue asscher\\n\\n\\n\\n\\n\\nsophist\\n\\n...\n",
       "2       207  on the decay of the art of lying\\n\\nby mark tw...\n",
       "3        53  cover \\n\\n\\n\\npericles, prince of tyre\\n\\n\\n\\n...\n",
       "4       183  cover \\n\\n\\n\\n\\nas you like it\\n\\nby william s...\n",
       "..      ...                                                ...\n",
       "431     210  produced by david widger\\n\\n\\n\\n\\n\\n          ...\n",
       "432     207  produced by anonymous volunteers, john greenma...\n",
       "433      10  transcriber's note:\\n\\nthe table of contents i...\n",
       "434     126  ﻿\\nproject gutenberg etext of the rape of lucr...\n",
       "435      33  produced by sue asscher\\n\\n\\n\\n\\n\\nstatesman\\n...\n",
       "\n",
       "[436 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Author', 'Book'], dtype='object')\n",
      "427 chunks created\n",
      "176 chunks created\n",
      "8 chunks created\n",
      "77 chunks created\n",
      "90 chunks created\n",
      "80 chunks created\n",
      "88 chunks created\n",
      "55 chunks created\n",
      "636 chunks created\n",
      "3757 chunks created\n",
      "285 chunks created\n",
      "12 chunks created\n",
      "116 chunks created\n",
      "119 chunks created\n",
      "535 chunks created\n",
      "39 chunks created\n",
      "84 chunks created\n",
      "96 chunks created\n",
      "275 chunks created\n",
      "364 chunks created\n",
      "46 chunks created\n",
      "91 chunks created\n",
      "82 chunks created\n",
      "84 chunks created\n",
      "57 chunks created\n",
      "95 chunks created\n",
      "104 chunks created\n",
      "87 chunks created\n",
      "92 chunks created\n",
      "431 chunks created\n",
      "844 chunks created\n",
      "110 chunks created\n",
      "36 chunks created\n",
      "115 chunks created\n",
      "540 chunks created\n",
      "89 chunks created\n",
      "32 chunks created\n",
      "540 chunks created\n",
      "115 chunks created\n",
      "86 chunks created\n",
      "103 chunks created\n",
      "121 chunks created\n",
      "32 chunks created\n",
      "111 chunks created\n",
      "207 chunks created\n",
      "617 chunks created\n",
      "52 chunks created\n",
      "26 chunks created\n",
      "75 chunks created\n",
      "79 chunks created\n",
      "115 chunks created\n",
      "107 chunks created\n",
      "258 chunks created\n",
      "68 chunks created\n",
      "663 chunks created\n",
      "38 chunks created\n",
      "505 chunks created\n",
      "69 chunks created\n",
      "88 chunks created\n",
      "23 chunks created\n",
      "109 chunks created\n",
      "312 chunks created\n",
      "33 chunks created\n",
      "843 chunks created\n",
      "4 chunks created\n",
      "31 chunks created\n",
      "94 chunks created\n",
      "248 chunks created\n",
      "56 chunks created\n",
      "96 chunks created\n",
      "75 chunks created\n",
      "91 chunks created\n",
      "432 chunks created\n",
      "27 chunks created\n",
      "89 chunks created\n",
      "108 chunks created\n",
      "78 chunks created\n",
      "59 chunks created\n",
      "15 chunks created\n",
      "70 chunks created\n",
      "41 chunks created\n",
      "1 chunks created\n",
      "41 chunks created\n",
      "96 chunks created\n",
      "24 chunks created\n",
      "124 chunks created\n",
      "10 chunks created\n",
      "108 chunks created\n",
      "440 chunks created\n",
      "256 chunks created\n",
      "650 chunks created\n",
      "70 chunks created\n",
      "552 chunks created\n",
      "108 chunks created\n",
      "83 chunks created\n",
      "35 chunks created\n",
      "60 chunks created\n",
      "41 chunks created\n",
      "51 chunks created\n",
      "120 chunks created\n",
      "325 chunks created\n",
      "133 chunks created\n",
      "139 chunks created\n",
      "108 chunks created\n",
      "140 chunks created\n",
      "15 chunks created\n",
      "90 chunks created\n",
      "158 chunks created\n",
      "317 chunks created\n",
      "596 chunks created\n",
      "145 chunks created\n",
      "44 chunks created\n",
      "107 chunks created\n",
      "2199 chunks created\n",
      "250 chunks created\n",
      "35 chunks created\n",
      "50 chunks created\n",
      "76 chunks created\n",
      "24 chunks created\n",
      "90 chunks created\n",
      "98 chunks created\n",
      "51 chunks created\n",
      "90 chunks created\n",
      "78 chunks created\n",
      "99 chunks created\n",
      "68 chunks created\n",
      "246 chunks created\n",
      "433 chunks created\n",
      "120 chunks created\n",
      "56 chunks created\n",
      "149 chunks created\n",
      "50 chunks created\n",
      "488 chunks created\n",
      "55 chunks created\n",
      "140 chunks created\n",
      "667 chunks created\n",
      "59 chunks created\n",
      "101 chunks created\n",
      "137 chunks created\n",
      "108 chunks created\n",
      "114 chunks created\n",
      "84 chunks created\n",
      "110 chunks created\n",
      "322 chunks created\n",
      "70 chunks created\n",
      "71 chunks created\n",
      "92 chunks created\n",
      "251 chunks created\n",
      "115 chunks created\n",
      "374 chunks created\n",
      "110 chunks created\n",
      "48 chunks created\n",
      "84 chunks created\n",
      "1386 chunks created\n",
      "186 chunks created\n",
      "604 chunks created\n",
      "1386 chunks created\n",
      "177 chunks created\n",
      "50 chunks created\n",
      "116 chunks created\n",
      "308 chunks created\n",
      "25 chunks created\n",
      "45 chunks created\n",
      "43 chunks created\n",
      "75 chunks created\n",
      "416 chunks created\n",
      "160 chunks created\n",
      "575 chunks created\n",
      "31 chunks created\n",
      "121 chunks created\n",
      "95 chunks created\n",
      "108 chunks created\n",
      "128 chunks created\n",
      "93 chunks created\n",
      "103 chunks created\n",
      "74 chunks created\n",
      "125 chunks created\n",
      "73 chunks created\n",
      "177 chunks created\n",
      "121 chunks created\n",
      "931 chunks created\n",
      "133 chunks created\n",
      "116 chunks created\n",
      "12 chunks created\n",
      "181 chunks created\n",
      "98 chunks created\n",
      "109 chunks created\n",
      "81 chunks created\n",
      "368 chunks created\n",
      "16 chunks created\n",
      "126 chunks created\n",
      "403 chunks created\n",
      "1 chunks created\n",
      "108 chunks created\n",
      "243 chunks created\n",
      "290 chunks created\n",
      "188 chunks created\n",
      "615 chunks created\n",
      "47 chunks created\n",
      "84 chunks created\n",
      "287 chunks created\n",
      "96 chunks created\n",
      "96 chunks created\n",
      "109 chunks created\n",
      "1392 chunks created\n",
      "79 chunks created\n",
      "93 chunks created\n",
      "3192 chunks created\n",
      "106 chunks created\n",
      "114 chunks created\n",
      "115 chunks created\n",
      "115 chunks created\n",
      "77 chunks created\n",
      "48 chunks created\n",
      "354 chunks created\n",
      "87 chunks created\n",
      "28 chunks created\n",
      "64 chunks created\n",
      "51 chunks created\n",
      "108 chunks created\n",
      "75 chunks created\n",
      "44 chunks created\n",
      "76 chunks created\n",
      "496 chunks created\n",
      "712 chunks created\n",
      "342 chunks created\n",
      "12 chunks created\n",
      "90 chunks created\n",
      "89 chunks created\n",
      "103 chunks created\n",
      "484 chunks created\n",
      "250 chunks created\n",
      "117 chunks created\n",
      "108 chunks created\n",
      "76 chunks created\n",
      "108 chunks created\n",
      "80 chunks created\n",
      "50 chunks created\n",
      "1317 chunks created\n",
      "85 chunks created\n",
      "58 chunks created\n",
      "71 chunks created\n",
      "47 chunks created\n",
      "653 chunks created\n",
      "125 chunks created\n",
      "206 chunks created\n",
      "17 chunks created\n",
      "1365 chunks created\n",
      "33 chunks created\n",
      "86 chunks created\n",
      "85 chunks created\n",
      "130 chunks created\n",
      "54 chunks created\n",
      "127 chunks created\n",
      "96 chunks created\n",
      "98 chunks created\n",
      "120 chunks created\n",
      "655 chunks created\n",
      "96 chunks created\n",
      "112 chunks created\n",
      "97 chunks created\n",
      "70 chunks created\n",
      "236 chunks created\n",
      "66 chunks created\n",
      "67 chunks created\n",
      "623 chunks created\n",
      "41 chunks created\n",
      "127 chunks created\n",
      "117 chunks created\n",
      "56 chunks created\n",
      "100 chunks created\n",
      "68 chunks created\n",
      "10884 chunks created\n",
      "68 chunks created\n",
      "420 chunks created\n",
      "52 chunks created\n",
      "309 chunks created\n",
      "96 chunks created\n",
      "96 chunks created\n",
      "92 chunks created\n",
      "754 chunks created\n",
      "110 chunks created\n",
      "97 chunks created\n",
      "98 chunks created\n",
      "96 chunks created\n",
      "92 chunks created\n",
      "106 chunks created\n",
      "45 chunks created\n",
      "81 chunks created\n",
      "400 chunks created\n",
      "299 chunks created\n",
      "114 chunks created\n",
      "49 chunks created\n",
      "59 chunks created\n",
      "459 chunks created\n",
      "22 chunks created\n",
      "486 chunks created\n",
      "51 chunks created\n",
      "74 chunks created\n",
      "43 chunks created\n",
      "65 chunks created\n",
      "114 chunks created\n",
      "90 chunks created\n",
      "87 chunks created\n",
      "71 chunks created\n",
      "306 chunks created\n",
      "95 chunks created\n",
      "108 chunks created\n",
      "91 chunks created\n",
      "105 chunks created\n",
      "94 chunks created\n",
      "119 chunks created\n",
      "30 chunks created\n",
      "51 chunks created\n",
      "76 chunks created\n",
      "111 chunks created\n",
      "132 chunks created\n",
      "109 chunks created\n",
      "118 chunks created\n",
      "90 chunks created\n",
      "38 chunks created\n",
      "99 chunks created\n",
      "3 chunks created\n",
      "355 chunks created\n",
      "83 chunks created\n",
      "191 chunks created\n",
      "33 chunks created\n",
      "38 chunks created\n",
      "12 chunks created\n",
      "13 chunks created\n",
      "84 chunks created\n",
      "79 chunks created\n",
      "96 chunks created\n",
      "103 chunks created\n",
      "555 chunks created\n",
      "77 chunks created\n",
      "225 chunks created\n",
      "100 chunks created\n",
      "8 chunks created\n",
      "101 chunks created\n",
      "31 chunks created\n",
      "138 chunks created\n",
      "301 chunks created\n",
      "82 chunks created\n",
      "54 chunks created\n",
      "92 chunks created\n",
      "101 chunks created\n",
      "330 chunks created\n",
      "135 chunks created\n",
      "74 chunks created\n",
      "94 chunks created\n",
      "74 chunks created\n",
      "88 chunks created\n",
      "37 chunks created\n",
      "0 chunks created\n",
      "7 chunks created\n",
      "122 chunks created\n",
      "91 chunks created\n",
      "80 chunks created\n",
      "27 chunks created\n",
      "105 chunks created\n",
      "87 chunks created\n",
      "96 chunks created\n",
      "31 chunks created\n",
      "617 chunks created\n",
      "209 chunks created\n",
      "104 chunks created\n",
      "81 chunks created\n",
      "98 chunks created\n",
      "74 chunks created\n",
      "739 chunks created\n",
      "338 chunks created\n",
      "93 chunks created\n",
      "17 chunks created\n",
      "71 chunks created\n",
      "92 chunks created\n",
      "10 chunks created\n",
      "91 chunks created\n",
      "85 chunks created\n",
      "31 chunks created\n",
      "63 chunks created\n",
      "33 chunks created\n",
      "107 chunks created\n",
      "89 chunks created\n",
      "97 chunks created\n",
      "23 chunks created\n",
      "40 chunks created\n",
      "113 chunks created\n",
      "362 chunks created\n",
      "277 chunks created\n",
      "63 chunks created\n",
      "96 chunks created\n",
      "32 chunks created\n",
      "90 chunks created\n",
      "573 chunks created\n",
      "115 chunks created\n",
      "231 chunks created\n",
      "93 chunks created\n",
      "96 chunks created\n",
      "213 chunks created\n",
      "112 chunks created\n",
      "108 chunks created\n",
      "78 chunks created\n",
      "92 chunks created\n",
      "91 chunks created\n",
      "88 chunks created\n",
      "61 chunks created\n",
      "184 chunks created\n",
      "154 chunks created\n",
      "30 chunks created\n",
      "85 chunks created\n",
      "58 chunks created\n",
      "102 chunks created\n",
      "44 chunks created\n",
      "977 chunks created\n",
      "38 chunks created\n",
      "84 chunks created\n",
      "22 chunks created\n",
      "1402 chunks created\n",
      "203 chunks created\n",
      "92 chunks created\n",
      "74 chunks created\n",
      "438 chunks created\n",
      "30 chunks created\n",
      "124 chunks created\n",
      "115 chunks created\n",
      "27 chunks created\n",
      "20 chunks created\n",
      "99 chunks created\n",
      "108 chunks created\n",
      "167 chunks created\n",
      "51 chunks created\n",
      "611 chunks created\n",
      "476 chunks created\n",
      "66 chunks created\n",
      "156 chunks created\n",
      "                             Author  \\\n",
      "0                   CHUNKTwain Mark   \n",
      "1                   CHUNKTwain Mark   \n",
      "2                   CHUNKTwain Mark   \n",
      "3                   CHUNKTwain Mark   \n",
      "4                   CHUNKTwain Mark   \n",
      "...                             ...   \n",
      "92892  CHUNKBurnett Frances Hodgson   \n",
      "92893  CHUNKBurnett Frances Hodgson   \n",
      "92894  CHUNKBurnett Frances Hodgson   \n",
      "92895  CHUNKBurnett Frances Hodgson   \n",
      "92896  CHUNKBurnett Frances Hodgson   \n",
      "\n",
      "                                                    Book  \n",
      "0      CHUNKproduced by betsie bush, chuck greif, mar...  \n",
      "1      CHUNKwhich he sees his own life reflected and ...  \n",
      "2      CHUNKbelieved it would, and asked me if i mean...  \n",
      "3      CHUNKtradition, this one or another--geoffrey ...  \n",
      "4      CHUNKof clemenses who said they had examined t...  \n",
      "...                                                  ...  \n",
      "92892  CHUNKone another, and give rise to a similar o...  \n",
      "92893  CHUNKshe desires to create, but only in what w...  \n",
      "92894  CHUNKgood and their opposites, which is true a...  \n",
      "92895  CHUNKmedicine which art prescribes for them, a...  \n",
      "92896  CHUNKwhereas they should both do precisely the...  \n",
      "\n",
      "[92897 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>207</td>\n",
       "      <td>produced by betsie bush, chuck greif, martin p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>207</td>\n",
       "      <td>which he sees his own life reflected and set d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>207</td>\n",
       "      <td>believed it would, and asked me if i meant to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>207</td>\n",
       "      <td>tradition, this one or another--geoffrey cleme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>207</td>\n",
       "      <td>of clemenses who said they had examined the re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92892</th>\n",
       "      <td>33</td>\n",
       "      <td>one another, and give rise to a similar opposi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92893</th>\n",
       "      <td>33</td>\n",
       "      <td>she desires to create, but only in what will p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92894</th>\n",
       "      <td>33</td>\n",
       "      <td>good and their opposites, which is true and co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92895</th>\n",
       "      <td>33</td>\n",
       "      <td>medicine which art prescribes for them, and of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92896</th>\n",
       "      <td>33</td>\n",
       "      <td>whereas they should both do precisely the oppo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92897 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Author                                               Book\n",
       "0         207  produced by betsie bush, chuck greif, martin p...\n",
       "1         207  which he sees his own life reflected and set d...\n",
       "2         207  believed it would, and asked me if i meant to ...\n",
       "3         207  tradition, this one or another--geoffrey cleme...\n",
       "4         207  of clemenses who said they had examined the re...\n",
       "...       ...                                                ...\n",
       "92892      33  one another, and give rise to a similar opposi...\n",
       "92893      33  she desires to create, but only in what will p...\n",
       "92894      33  good and their opposites, which is true and co...\n",
       "92895      33  medicine which art prescribes for them, and of...\n",
       "92896      33  whereas they should both do precisely the oppo...\n",
       "\n",
       "[92897 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_init = read_csv(config.DATA_PROCESSED_PATH + 'books_processed.csv')\n",
    "df = preprocessor.body_preprocessor(df_init)\n",
    "df = preprocessor.clean_body_preprocessor(df)\n",
    "df = preprocessor.chunk_text_preprocessor(df)\n",
    "df = preprocessor.encode_label_preprocessor(df)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df to csv \n",
    "df.to_csv(config.DATA_FINAL_PATH + 'books_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec embedding\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "wordtovec = Word2Vec(df['Book'], window=10, workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>207</td>\n",
       "      <td>[[-0.14814544, 0.17401853, -0.41310522, -0.211...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>207</td>\n",
       "      <td>[[0.6881956, 0.99037874, 0.2590469, 0.265691, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>207</td>\n",
       "      <td>[[0.4794665, 0.52229464, 0.0414591, 0.8494552,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>207</td>\n",
       "      <td>[[0.23006427, 0.5083755, 0.17851238, -0.018575...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>207</td>\n",
       "      <td>[[0.46350753, 0.66330725, 0.61609185, 0.204386...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92892</th>\n",
       "      <td>33</td>\n",
       "      <td>[[0.46350753, 0.66330725, 0.61609185, 0.204386...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92893</th>\n",
       "      <td>33</td>\n",
       "      <td>[[0.17210265, 0.5945774, 0.16267185, 0.2602231...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92894</th>\n",
       "      <td>33</td>\n",
       "      <td>[[-0.038096186, 0.8723225, 0.16824491, 1.13946...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92895</th>\n",
       "      <td>33</td>\n",
       "      <td>[[-0.008179311, 1.0293553, 0.37904462, 0.29931...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92896</th>\n",
       "      <td>33</td>\n",
       "      <td>[[0.6881956, 0.99037874, 0.2590469, 0.265691, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92897 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Author                                               Book\n",
       "0         207  [[-0.14814544, 0.17401853, -0.41310522, -0.211...\n",
       "1         207  [[0.6881956, 0.99037874, 0.2590469, 0.265691, ...\n",
       "2         207  [[0.4794665, 0.52229464, 0.0414591, 0.8494552,...\n",
       "3         207  [[0.23006427, 0.5083755, 0.17851238, -0.018575...\n",
       "4         207  [[0.46350753, 0.66330725, 0.61609185, 0.204386...\n",
       "...       ...                                                ...\n",
       "92892      33  [[0.46350753, 0.66330725, 0.61609185, 0.204386...\n",
       "92893      33  [[0.17210265, 0.5945774, 0.16267185, 0.2602231...\n",
       "92894      33  [[-0.038096186, 0.8723225, 0.16824491, 1.13946...\n",
       "92895      33  [[-0.008179311, 1.0293553, 0.37904462, 0.29931...\n",
       "92896      33  [[0.6881956, 0.99037874, 0.2590469, 0.265691, ...\n",
       "\n",
       "[92897 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_da_shit(words):\n",
    "    return [word for word in words if word not in ['ἅ', '•', 'ÿ', 'ὦ', 'ἠ', '×', '°', 'ß', '᾽', 'š', '\\x92', 'ỏ']]\n",
    "\n",
    "df['Book'] = df['Book'].apply(remove_da_shit)\n",
    "\n",
    "df['Book'] = df['Book'].apply(lambda x: [wordtovec.wv[word] for word in x if word in wordtovec.wv])\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df to csv\n",
    "\n",
    "df.to_csv(config.DATA_FINAL_PATH + 'books_preprocessed_vec.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ywl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
